import requests
import json
import time
import os
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MultiLanguageSemanticAnalyzer:
    """å¤šè¯­è¨€ä»£ç è¯­ä¹‰åˆ†æå™¨"""
    
    # è¯­è¨€é…ç½®
    LANGUAGE_CONFIG = {
        'python': {
            'file_extensions': ['.py'],
            'import_patterns': [
                r'^\s*import\s+(.+)',
                r'^\s*from\s+([^\s]+)\s+import\s+(.+)'
            ],
            'function_patterns': [
                r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
            ],
            'class_patterns': [
                r'class\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[\(:]'
            ],
            'comment_patterns': [r'#.*$', r'""".*?"""', r"'''.*?'''"]
        },
        'javascript': {
            'file_extensions': ['.js', '.jsx', '.ts', '.tsx'],
            'import_patterns': [
                r'^\s*import\s+(.+?)\s+from\s+["\']([^"\']+)["\']',
                r'^\s*import\s+["\']([^"\']+)["\']',
                r'^\s*const\s+(.+?)\s*=\s*require\s*\(\s*["\']([^"\']+)["\']\s*\)',
                r'^\s*require\s*\(\s*["\']([^"\']+)["\']\s*\)'
            ],
            'function_patterns': [
                r'function\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*\(',
                r'const\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*=\s*\(',
                r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*:\s*function\s*\(',
                r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*\([^)]*\)\s*=>'
            ],
            'class_patterns': [
                r'class\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*[{(]'
            ],
            'comment_patterns': [r'//.*$', r'/\*.*?\*/']
        },
        'java': {
            'file_extensions': ['.java'],
            'import_patterns': [
                r'^\s*import\s+(?:static\s+)?([^;]+);'
            ],
            'function_patterns': [
                r'(?:public|private|protected|static|\s)*\s+\w+\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
            ],
            'class_patterns': [
                r'(?:public|private|protected|abstract|final|\s)*\s*class\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[{<]'
            ],
            'comment_patterns': [r'//.*$', r'/\*.*?\*/']
        },
        'golang': {
            'file_extensions': ['.go'],
            'import_patterns': [
                r'^\s*import\s+"([^"]+)"',
                r'^\s*import\s+([a-zA-Z_][a-zA-Z0-9_]*)\s+"([^"]+)"',
                r'^\s*import\s+\(\s*\n((?:\s*"[^"]+"\s*\n)*)\s*\)'
            ],
            'function_patterns': [
                r'func\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(',
                r'func\s+\([^)]*\)\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
            ],
            'class_patterns': [
                r'type\s+([a-zA-Z_][a-zA-Z0-9_]*)\s+struct\s*{'
            ],
            'comment_patterns': [r'//.*$', r'/\*.*?\*/']
        },
        'cpp': {
            'file_extensions': ['.cpp', '.cc', '.cxx', '.c++', '.hpp', '.h'],
            'import_patterns': [
                r'^\s*#include\s*<([^>]+)>',
                r'^\s*#include\s*"([^"]+)"',
                r'^\s*using\s+namespace\s+([^;]+);'
            ],
            'function_patterns': [
                r'(?:inline\s+)?(?:static\s+)?(?:virtual\s+)?(?:const\s+)?\w+\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(',
                r'([a-zA-Z_][a-zA-Z0-9_]*)::[a-zA-Z_][a-zA-Z0-9_]*\s*\('
            ],
            'class_patterns': [
                r'class\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[{:]',
                r'struct\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[{:]'
            ],
            'comment_patterns': [r'//.*$', r'/\*.*?\*/']
        },
        'typescript': {
            'file_extensions': ['.ts', '.tsx'],
            'import_patterns': [
                r'^\s*import\s+(.+?)\s+from\s+["\']([^"\']+)["\']',
                r'^\s*import\s+["\']([^"\']+)["\']',
                r'^\s*import\s+type\s+(.+?)\s+from\s+["\']([^"\']+)["\']'
            ],
            'function_patterns': [
                r'function\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*\(',
                r'const\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*=\s*\(',
                r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*:\s*function\s*\(',
                r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*\([^)]*\)\s*=>'
            ],
            'class_patterns': [
                r'class\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*[{<]',
                r'interface\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*[{<]'
            ],
            'comment_patterns': [r'//.*$', r'/\*.*?\*/']
        }
    }
    
    @staticmethod
    def get_language_from_file(file_path: str) -> Optional[str]:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­ç¼–ç¨‹è¯­è¨€"""
        ext = Path(file_path).suffix.lower()
        
        for language, config in MultiLanguageSemanticAnalyzer.LANGUAGE_CONFIG.items():
            if ext in config['file_extensions']:
                return language
        return None
    
    @staticmethod
    def extract_imports(file_content: str, language: str) -> List[Dict[str, Any]]:
        """æå–æŒ‡å®šè¯­è¨€çš„å¯¼å…¥è¯­å¥"""
        if language not in MultiLanguageSemanticAnalyzer.LANGUAGE_CONFIG:
            return []
        
        config = MultiLanguageSemanticAnalyzer.LANGUAGE_CONFIG[language]
        imports = []
        lines = file_content.split('\n')
        
        for line_num, line in enumerate(lines, 1):
            stripped = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not stripped:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ³¨é‡Š
            is_comment = False
            for comment_pattern in config['comment_patterns']:
                if re.search(comment_pattern, stripped):
                    is_comment = True
                    break
            
            if is_comment:
                continue
            
            # åŒ¹é…å¯¼å…¥è¯­å¥
            for pattern in config['import_patterns']:
                match = re.search(pattern, stripped)
                if match:
                    import_info = {
                        'language': language,
                        'line_number': line_num,
                        'import_statement': stripped,
                        'match_groups': match.groups()
                    }
                    
                    # æ ¹æ®è¯­è¨€ç‰¹å®šå¤„ç†
                    if language == 'python':
                        if 'from' in stripped:
                            import_info['import_type'] = 'from_import'
                            import_info['module_name'] = match.group(1)
                            import_info['imported_items'] = [item.strip() for item in match.group(2).split(',')]
                        else:
                            import_info['import_type'] = 'import'
                            import_info['module_name'] = match.group(1)
                            import_info['imported_items'] = None
                    
                    elif language in ['javascript', 'typescript']:
                        if 'from' in stripped:
                            import_info['import_type'] = 'es6_import'
                            import_info['imported_items'] = match.group(1).strip()
                            import_info['module_name'] = match.group(2)
                        elif 'require' in stripped:
                            import_info['import_type'] = 'require'
                            import_info['module_name'] = match.group(1) if len(match.groups()) == 1 else match.group(2)
                    
                    elif language == 'java':
                        import_info['import_type'] = 'import'
                        import_info['module_name'] = match.group(1)
                    
                    elif language == 'golang':
                        import_info['import_type'] = 'import'
                        import_info['module_name'] = match.group(1)
                    
                    elif language == 'cpp':
                        if '<' in stripped:
                            import_info['import_type'] = 'system_include'
                            import_info['module_name'] = match.group(1)
                        elif '"' in stripped:
                            import_info['import_type'] = 'local_include'
                            import_info['module_name'] = match.group(1)
                        elif 'using' in stripped:
                            import_info['import_type'] = 'using_namespace'
                            import_info['module_name'] = match.group(1)
                    
                    imports.append(import_info)
                    break
        
        return imports
    
    @staticmethod
    def detect_function_changes(patch_content: str, language: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹å‡½æ•°å˜æ›´ï¼ˆå¤šè¯­è¨€æ”¯æŒï¼‰"""
        if language not in MultiLanguageSemanticAnalyzer.LANGUAGE_CONFIG:
            return []
        
        config = MultiLanguageSemanticAnalyzer.LANGUAGE_CONFIG[language]
        functions = []
        hunks = MultiLanguageSemanticAnalyzer.parse_diff_hunks(patch_content)
        
        for hunk in hunks:
            # æ£€æŸ¥hunkçš„contextä¿¡æ¯
            context = hunk.get('context', '')
            for pattern in config['function_patterns']:
                match = re.search(pattern, context)
                if match:
                    functions.append({
                        'function_name': match.group(1),
                        'change_type': 'modified',
                        'line_content': context.strip(),
                        'source': 'context',
                        'language': language
                    })
            
            # æ£€æŸ¥å˜æ›´å†…å®¹ä¸­çš„å‡½æ•°å®šä¹‰
            for change_line in hunk['changes']:
                if change_line.startswith(('+', '-')):
                    for pattern in config['function_patterns']:
                        match = re.search(pattern, change_line)
                        if match:
                            change_type = 'added' if change_line.startswith('+') else 'removed'
                            functions.append({
                                'function_name': match.group(1),
                                'change_type': change_type,
                                'line_content': change_line[1:].strip(),
                                'source': 'diff_content',
                                'language': language
                            })
        
        # å»é‡
        unique_functions = {}
        for func in functions:
            key = f"{func['function_name']}_{func['language']}"
            if key not in unique_functions:
                unique_functions[key] = func
            else:
                if func['source'] == 'diff_content':
                    unique_functions[key] = func
        
        return list(unique_functions.values())
    
    @staticmethod
    def detect_class_changes(patch_content: str, language: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹ç±»å˜æ›´ï¼ˆå¤šè¯­è¨€æ”¯æŒï¼‰"""
        if language not in MultiLanguageSemanticAnalyzer.LANGUAGE_CONFIG:
            return []
        
        config = MultiLanguageSemanticAnalyzer.LANGUAGE_CONFIG[language]
        classes = []
        hunks = MultiLanguageSemanticAnalyzer.parse_diff_hunks(patch_content)
        
        for hunk in hunks:
            # æ£€æŸ¥hunkçš„contextä¿¡æ¯
            context = hunk.get('context', '')
            for pattern in config['class_patterns']:
                match = re.search(pattern, context)
                if match:
                    classes.append({
                        'class_name': match.group(1),
                        'change_type': 'modified',
                        'line_content': context.strip(),
                        'source': 'context',
                        'language': language
                    })
            
            # æ£€æŸ¥å˜æ›´å†…å®¹ä¸­çš„ç±»å®šä¹‰
            for change_line in hunk['changes']:
                if change_line.startswith(('+', '-')):
                    for pattern in config['class_patterns']:
                        match = re.search(pattern, change_line)
                        if match:
                            change_type = 'added' if change_line.startswith('+') else 'removed'
                            classes.append({
                                'class_name': match.group(1),
                                'change_type': change_type,
                                'line_content': change_line[1:].strip(),
                                'source': 'diff_content',
                                'language': language
                            })
        
        # å»é‡
        unique_classes = {}
        for cls in classes:
            key = f"{cls['class_name']}_{cls['language']}"
            if key not in unique_classes:
                unique_classes[key] = cls
            else:
                if cls['source'] == 'diff_content':
                    unique_classes[key] = cls
        
        return list(unique_classes.values())
    
    @staticmethod
    def parse_diff_hunks(patch_content: str) -> List[Dict[str, Any]]:
        """è§£ædiffå†…å®¹ä¸ºhunks"""
        hunks = []
        current_hunk = None
        
        for line in patch_content.split('\n'):
            if line.startswith('@@'):
                match = re.match(r'@@ -(\d+),?(\d+)? \+(\d+),?(\d+)? @@(.+)?', line)
                if match:
                    old_start = int(match.group(1))
                    old_count = int(match.group(2) or 1)
                    new_start = int(match.group(3))
                    new_count = int(match.group(4) or 1)
                    context = match.group(5) or ''
                    
                    current_hunk = {
                        'old_start': old_start,
                        'old_count': old_count,
                        'new_start': new_start,
                        'new_count': new_count,
                        'context': context.strip(),
                        'changes': []
                    }
                    hunks.append(current_hunk)
            
            elif current_hunk and line.startswith(('+', '-', ' ')):
                current_hunk['changes'].append(line)
        
        return hunks

class MultiLanguageGitHubPRCrawler:
    def __init__(self, token: str, repos_dir: str = "top_2000_star_repos_this_year", 
                 output_dir: str = "github_pr_data"):
        """
        åˆå§‹åŒ–å¤šè¯­è¨€ GitHub PR çˆ¬è™«
        
        Args:
            token: GitHub Personal Access Token
            repos_dir: å­˜æ”¾ä»“åº“åˆ—è¡¨JSONLæ–‡ä»¶çš„ç›®å½•
            output_dir: è¾“å‡ºæ•°æ®çš„ç›®å½•
        """
        self.token = token
        self.headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'GitHub-PR-Crawler'
        }
        self.base_url = 'https://api.github.com'
        self.repos_dir = Path(repos_dir)
        self.output_dir = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å¤šè¯­è¨€è¯­ä¹‰åˆ†æå™¨
        self.analyzer = MultiLanguageSemanticAnalyzer()
        
        # API è°ƒç”¨è®¡æ•°å’Œé™åˆ¶
        self.api_calls = 0
        self.rate_limit_remaining = 5000
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_repos_attempted': 0,
            'repos_successfully_processed': 0,
            'repos_skipped_no_prs': 0,
            'repos_skipped_too_many_prs': 0,
            'repos_failed': 0,
            'total_prs_processed': 0,
            'repos_skipped_already_processed': 0,
            'functions_detected': 0,
            'classes_detected': 0,
            'imports_extracted': 0,
            'language_stats': {}
        }
    
    def check_rate_limit(self) -> None:
        """æ£€æŸ¥å¹¶å¤„ç† API é€Ÿç‡é™åˆ¶"""
        if self.rate_limit_remaining < 50:
            logger.warning(f"API rate limit low: {self.rate_limit_remaining}, waiting...")
            time.sleep(60)
    
    def make_request(self, url: str, params: Dict = None, max_retries: int = 3) -> Optional[Dict]:
        """å‘é€ API è¯·æ±‚å¹¶å¤„ç†å“åº”"""
        for attempt in range(max_retries):
            self.check_rate_limit()
            
            try:
                response = requests.get(url, headers=self.headers, params=params, timeout=30)
                self.api_calls += 1
                
                self.rate_limit_remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
                rate_limit_reset = response.headers.get('X-RateLimit-Reset', 0)
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 403:
                    logger.error("Rate limit exceeded, waiting...")
                    reset_time = int(rate_limit_reset) - int(time.time())
                    wait_time = max(reset_time, 3600)
                    time.sleep(wait_time)
                    return self.make_request(url, params, max_retries=1)
                elif response.status_code == 404:
                    logger.warning(f"Resource not found: {url}")
                    return None
                elif response.status_code in [502, 503, 504]:
                    logger.warning(f"Server error {response.status_code}, retrying in {2 ** attempt} seconds...")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    logger.error(f"API request failed: {response.status_code} - {response.text}")
                    return None
                    
            except Exception as e:
                logger.error(f"Request failed (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    return None
    
    def load_repos_from_file(self, language: str, min_stars: int = 1000, 
                           start_index: int = 0, batch_size: int = 50) -> List[Dict]:
        """ä»JSONLæ–‡ä»¶ä¸­åŠ è½½æŒ‡å®šè¯­è¨€çš„ä»“åº“åˆ—è¡¨"""
        # è¯­è¨€æ˜ å°„
        language_mapping = {
            'cpp': 'c++',
            'golang': 'go'
        }
        
        file_language = language_mapping.get(language, language)
        file_path = self.repos_dir / f"top_{file_language}_stars_this_year.jsonl"
        
        if not file_path.exists():
            logger.error(f"Repository file not found: {file_path}")
            return []
        
        logger.info(f"Loading repositories from: {file_path} (starting from index {start_index}, batch size {batch_size})")
        
        repos = []
        current_index = 0
        loaded_count = 0
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        repo_data = json.loads(line.strip())
                        
                        if repo_data.get('star_count', 0) >= min_stars:
                            if current_index >= start_index:
                                repo_name = repo_data['repo_name']
                                if '/' in repo_name:
                                    owner, name = repo_name.split('/', 1)
                                    
                                    repo_info = {
                                        'owner': {'login': owner},
                                        'name': name,
                                        'full_name': repo_name,
                                        'language': repo_data.get('language'),
                                        'stargazers_count': repo_data.get('star_count', 0),
                                        'html_url': f"https://github.com/{repo_name}",
                                        'pushed_at': repo_data.get('latest_pushed_time')
                                    }
                                    repos.append(repo_info)
                                    loaded_count += 1
                                    
                                    if loaded_count >= batch_size:
                                        break
                            
                            current_index += 1
            
            logger.info(f"Loaded {len(repos)} repositories for {language}")
            return repos
            
        except Exception as e:
            logger.error(f"Error loading repositories from {file_path}: {e}")
            return []
    
    def save_to_jsonl(self, data: Dict, file_path: Path):
        """ä¿å­˜æ•°æ®åˆ°JSONLæ–‡ä»¶"""
        try:
            with open(file_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(data, ensure_ascii=False) + '\n')
        except Exception as e:
            logger.error(f"Error saving to {file_path}: {e}")
    
    def get_processed_repos_set(self, language: str) -> set:
        """è·å–å·²å¤„ç†çš„ä»“åº“é›†åˆ"""
        processed_repos = set()
        
        # æ£€æŸ¥å„ç§è¾“å‡ºæ–‡ä»¶
        output_files = [
            self.output_dir / f"{language}_pr_data.jsonl",
            self.output_dir / f"{language}_commits.jsonl",
            self.output_dir / f"{language}_file_changes.jsonl",
            self.output_dir / f"{language}_function_changes.jsonl",
            self.output_dir / f"{language}_class_changes.jsonl",
            self.output_dir / f"{language}_imports.jsonl"
        ]
        
        for file_path in output_files:
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                data = json.loads(line.strip())
                                repo_full_name = data.get('repo_full_name')
                                if repo_full_name:
                                    processed_repos.add(repo_full_name)
                except Exception as e:
                    logger.warning(f"Error reading {file_path}: {e}")
        
        logger.info(f"Found {len(processed_repos)} already processed repos")
        return processed_repos
    
    def get_file_content_at_commit(self, owner: str, repo_name: str, file_path: str, commit_sha: str) -> Optional[str]:
        """è·å–æ–‡ä»¶åœ¨ç‰¹å®šcommitæ—¶é—´ç‚¹çš„å†…å®¹"""
        try:
            url = f"{self.base_url}/repos/{owner}/{repo_name}/contents/{file_path}"
            params = {'ref': commit_sha}
            
            data = self.make_request(url, params)
            if not data:
                return None
            
            # GitHub APIè¿”å›base64ç¼–ç çš„å†…å®¹
            if data.get('encoding') == 'base64':
                import base64
                content = base64.b64decode(data['content']).decode('utf-8', errors='ignore')
                return content
            
        except Exception as e:
            logger.warning(f"Error getting file content at commit {commit_sha[:8]}: {e}")
        return None
    
    def get_pull_requests(self, owner: str, repo: str, state: str = 'closed', 
                         max_prs: int = None, skip_if_too_many: int = 100) -> List[Dict]:
        """è·å–ä»“åº“çš„æ‰€æœ‰ Pull Requests"""
        logger.info(f"Getting all PRs for {owner}/{repo}")
        
        prs = []
        page = 1
        
        while True:
            url = f"{self.base_url}/repos/{owner}/{repo}/pulls"
            params = {
                'state': state,
                'sort': 'updated',
                'direction': 'desc',
                'per_page': 100,
                'page': page
            }
            
            data = self.make_request(url, params)
            if not data:
                break
            
            merged_prs = [pr for pr in data if pr.get('merged_at')]
            prs.extend(merged_prs)
            
            logger.info(f"Found {len(merged_prs)} merged PRs on page {page}, total: {len(prs)}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡
            if page == 1 and len(merged_prs) > 80 and len(data) == 100:
                estimated_total = len(merged_prs) * 10
                if estimated_total > skip_if_too_many:
                    logger.warning(f"Skipping {owner}/{repo} - estimated {estimated_total} merged PRs")
                    self.stats['repos_skipped_too_many_prs'] += 1
                    return []
            
            if len(prs) > skip_if_too_many:
                logger.warning(f"Skipping {owner}/{repo} - found {len(prs)} merged PRs")
                self.stats['repos_skipped_too_many_prs'] += 1
                return []
            
            if len(data) < 100:
                break
            
            if max_prs and len(prs) >= max_prs:
                prs = prs[:max_prs]
                break
            
            page += 1
            time.sleep(0.5)
        
        logger.info(f"Found total {len(prs)} merged PRs for {owner}/{repo}")
        return prs
    
    def get_pr_commits(self, owner: str, repo: str, pr_number: int) -> List[Dict]:
        """è·å– PR çš„æ‰€æœ‰ commits"""
        commits = []
        page = 1
        
        while True:
            url = f"{self.base_url}/repos/{owner}/{repo}/pulls/{pr_number}/commits"
            params = {'per_page': 100, 'page': page}
            
            data = self.make_request(url, params)
            if not data:
                break
            
            commits.extend(data)
            
            if len(data) < 100:
                break
            
            page += 1
            time.sleep(0.2)
        
        return commits
    
    def get_pr_files(self, owner: str, repo: str, pr_number: int) -> List[Dict]:
        """è·å– PR ä¿®æ”¹çš„æ–‡ä»¶"""
        files = []
        page = 1
        
        while True:
            url = f"{self.base_url}/repos/{owner}/{repo}/pulls/{pr_number}/files"
            params = {'per_page': 100, 'page': page}
            
            data = self.make_request(url, params)
            if not data:
                break
            
            files.extend(data)
            
            if len(data) < 100:
                break
            
            page += 1
            time.sleep(0.2)
        
        return files
    
    def get_pr_reviews(self, owner: str, repo: str, pr_number: int) -> List[Dict]:
        """è·å– PR çš„ reviews"""
        reviews = []
        page = 1
        
        while True:
            url = f"{self.base_url}/repos/{owner}/{repo}/pulls/{pr_number}/reviews"
            params = {'per_page': 100, 'page': page}
            
            data = self.make_request(url, params)
            if not data:
                break
            
            reviews.extend(data)
            
            if len(data) < 100:
                break
            
            page += 1
            time.sleep(0.2)
        
        return reviews
    
    def get_pr_review_comments(self, owner: str, repo: str, pr_number: int) -> List[Dict]:
        """è·å– PR çš„ review comments"""
        review_comments = []
        page = 1
        
        while True:
            url = f"{self.base_url}/repos/{owner}/{repo}/pulls/{pr_number}/comments"
            params = {'per_page': 100, 'page': page}
            
            data = self.make_request(url, params)
            if not data:
                break
            
            review_comments.extend(data)
            
            if len(data) < 100:
                break
            
            page += 1
            time.sleep(0.2)
        
        return review_comments
    
    def get_commit_details(self, owner: str, repo: str, sha: str) -> Optional[Dict]:
        """è·å– commit çš„è¯¦ç»†ä¿¡æ¯"""
        url = f"{self.base_url}/repos/{owner}/{repo}/commits/{sha}"
        return self.make_request(url)
    
    def process_pr_data(self, repo_info: Dict, pr_data: Dict, language: str) -> bool:
        """å¤„ç†å•ä¸ª PR çš„å®Œæ•´æ•°æ®ï¼Œä¿å­˜åˆ°JSONLæ–‡ä»¶"""
        owner = repo_info['owner']['login']
        repo_name = repo_info['name']
        repo_full_name = repo_info['full_name']
        pr_number = pr_data['number']
        
        logger.info(f"Processing PR #{pr_number} in {repo_full_name}")
        
        try:
            # è·å– PR ç›¸å…³æ•°æ®
            commits = self.get_pr_commits(owner, repo_name, pr_number)
            files = self.get_pr_files(owner, repo_name, pr_number)
            reviews = self.get_pr_reviews(owner, repo_name, pr_number)
            review_comments = self.get_pr_review_comments(owner, repo_name, pr_number)
            
            if not commits:
                logger.warning(f"No commits found for PR #{pr_number}")
                return False
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'additions': sum([f.get('additions', 0) for f in files]),
                'deletions': sum([f.get('deletions', 0) for f in files]),
                'changed_files': len(files),
                'commits_count': len(commits),
                'reviews_count': len(reviews),
                'review_comments_count': len(review_comments)
            }
            
            # ä¿å­˜PRåŸºæœ¬ä¿¡æ¯
            pr_record = {
                'repo_full_name': repo_full_name,
                'repo_language': repo_info.get('language'),
                'repo_stars': repo_info.get('stargazers_count', 0),
                'pr_number': pr_number,
                'pr_title': pr_data.get('title', ''),
                'pr_body': pr_data.get('body', ''),
                'pr_author': pr_data.get('user', {}).get('login', ''),
                'pr_created_at': pr_data.get('created_at'),
                'pr_merged_at': pr_data.get('merged_at'),
                'pr_stats': stats,
                'processed_at': datetime.now().isoformat()
            }
            
            self.save_to_jsonl(pr_record, self.output_dir / f"{language}_pr_data.jsonl")
            
            # ä¿å­˜å®¡æŸ¥è¯„è®º
            for review in reviews:
                if review.get('body'):
                    review_record = {
                        'repo_full_name': repo_full_name,
                        'pr_number': pr_number,
                        'comment_type': 'review',
                        'reviewer': review.get('user', {}).get('login', ''),
                        'comment_text': review.get('body', ''),
                        'state': review.get('state', ''),
                        'created_at': review.get('submitted_at')
                    }
                    self.save_to_jsonl(review_record, self.output_dir / f"{language}_review_comments.jsonl")
            
            for comment in review_comments:
                if comment.get('body'):
                    comment_record = {
                        'repo_full_name': repo_full_name,
                        'pr_number': pr_number,
                        'comment_type': 'review_comment',
                        'reviewer': comment.get('user', {}).get('login', ''),
                        'comment_text': comment.get('body', ''),
                        'file_path': comment.get('path', ''),
                        'line_number': comment.get('line'),
                        'created_at': comment.get('created_at')
                    }
                    self.save_to_jsonl(comment_record, self.output_dir / f"{language}_review_comments.jsonl")
            
            # å¤„ç†æ¯ä¸ª commit
            for commit in commits:
                commit_hash = commit['sha']
                
                commit_details = self.get_commit_details(owner, repo_name, commit_hash)
                if not commit_details:
                    continue
                
                # ä¿å­˜commitä¿¡æ¯
                commit_record = {
                    'repo_full_name': repo_full_name,
                    'pr_number': pr_number,
                    'commit_hash': commit_hash,
                    'commit_message': commit.get('commit', {}).get('message', ''),
                    'commit_author': commit.get('commit', {}).get('author', {}).get('name', ''),
                    'commit_author_email': commit.get('commit', {}).get('author', {}).get('email', ''),
                    'committed_at': commit.get('commit', {}).get('committer', {}).get('date'),
                    'commit_stats': commit_details.get('stats', {})
                }
                
                self.save_to_jsonl(commit_record, self.output_dir / f"{language}_commits.jsonl")
                
                # å¤„ç†æ–‡ä»¶å˜æ›´
                commit_files = commit_details.get('files', [])
                for file_data in commit_files:
                    file_path = file_data.get('filename', '')
                    
                    # åˆ¤æ–­æ–‡ä»¶çš„ç¼–ç¨‹è¯­è¨€
                    file_language = self.analyzer.get_language_from_file(file_path)
                    
                    # ä¿å­˜æ–‡ä»¶å˜æ›´ä¿¡æ¯
                    file_change_record = {
                        'repo_full_name': repo_full_name,
                        'pr_number': pr_number,
                        'commit_hash': commit_hash,
                        'file_path': file_path,
                        'file_language': file_language,
                        'change_type': file_data.get('status', 'modified'),
                        'additions': file_data.get('additions', 0),
                        'deletions': file_data.get('deletions', 0),
                        'changes': file_data.get('changes', 0),
                        'patch_content': file_data.get('patch', '')
                    }
                    
                    self.save_to_jsonl(file_change_record, self.output_dir / f"{language}_file_changes.jsonl")
                    
                    # å¦‚æœæ–‡ä»¶æœ‰å¯¹åº”çš„è¯­è¨€ï¼Œè¿›è¡Œè¯­ä¹‰åˆ†æ
                    if file_language:
                        patch_content = file_data.get('patch', '')
                        if patch_content:
                            # åˆ†æå‡½æ•°å˜æ›´
                            functions = self.analyzer.detect_function_changes(patch_content, file_language)
                            for func in functions:
                                func_record = {
                                    'repo_full_name': repo_full_name,
                                    'pr_number': pr_number,
                                    'commit_hash': commit_hash,
                                    'file_path': file_path,
                                    'file_language': file_language,
                                    'function_name': func['function_name'],
                                    'change_type': func['change_type'],
                                    'line_content': func['line_content'],
                                    'source': func['source']
                                }
                                self.save_to_jsonl(func_record, self.output_dir / f"{language}_function_changes.jsonl")
                                self.stats['functions_detected'] += 1
                            
                            # åˆ†æç±»å˜æ›´
                            classes = self.analyzer.detect_class_changes(patch_content, file_language)
                            for cls in classes:
                                class_record = {
                                    'repo_full_name': repo_full_name,
                                    'pr_number': pr_number,
                                    'commit_hash': commit_hash,
                                    'file_path': file_path,
                                    'file_language': file_language,
                                    'class_name': cls['class_name'],
                                    'change_type': cls['change_type'],
                                    'line_content': cls['line_content'],
                                    'source': cls['source']
                                }
                                self.save_to_jsonl(class_record, self.output_dir / f"{language}_class_changes.jsonl")
                                self.stats['classes_detected'] += 1
                            
                            # ä¿å­˜diff hunks
                            hunks = self.analyzer.parse_diff_hunks(patch_content)
                            for i, hunk in enumerate(hunks):
                                hunk_record = {
                                    'repo_full_name': repo_full_name,
                                    'pr_number': pr_number,
                                    'commit_hash': commit_hash,
                                    'file_path': file_path,
                                    'file_language': file_language,
                                    'hunk_index': i,
                                    'old_start': hunk['old_start'],
                                    'old_count': hunk['old_count'],
                                    'new_start': hunk['new_start'],
                                    'new_count': hunk['new_count'],
                                    'context': hunk['context'],
                                    'content': '\n'.join(hunk['changes'])
                                }
                                self.save_to_jsonl(hunk_record, self.output_dir / f"{language}_diff_hunks.jsonl")
                        
                        # æå–importä¿¡æ¯
                        file_content = self.get_file_content_at_commit(owner, repo_name, file_path, commit_hash)
                        if file_content:
                            imports = self.analyzer.extract_imports(file_content, file_language)
                            for imp in imports:
                                import_record = {
                                    'repo_full_name': repo_full_name,
                                    'pr_number': pr_number,
                                    'commit_hash': commit_hash,
                                    'file_path': file_path,
                                    'file_language': file_language,
                                    'import_statement': imp['import_statement'],
                                    'import_type': imp['import_type'],
                                    'module_name': imp['module_name'],
                                    'imported_items': imp.get('imported_items'),
                                    'line_number': imp['line_number']
                                }
                                self.save_to_jsonl(import_record, self.output_dir / f"{language}_imports.jsonl")
                                self.stats['imports_extracted'] += 1
                    
                    # æ›´æ–°è¯­è¨€ç»Ÿè®¡
                    if file_language:
                        if file_language not in self.stats['language_stats']:
                            self.stats['language_stats'][file_language] = 0
                        self.stats['language_stats'][file_language] += 1
                
                time.sleep(0.3)
            
            logger.info(f"Successfully processed PR #{pr_number} with {len(commits)} commits")
            return True
            
        except Exception as e:
            logger.error(f"Error processing PR #{pr_number}: {e}")
            return False
    
    def crawl_language(self, language: str, target_repos: int = 200, max_prs_per_repo: int = None):
        """çˆ¬å–æŒ‡å®šè¯­è¨€çš„æ•°æ®å¹¶ä¿å­˜åˆ°JSONLæ–‡ä»¶"""
        logger.info(f"Starting crawl for language: {language}, target repos: {target_repos}")
        
        # è·å–å·²å¤„ç†çš„ä»“åº“é›†åˆ
        processed_repos_set = self.get_processed_repos_set(language)
        
        successfully_processed_repos = len(processed_repos_set)
        logger.info(f"Resume from previous progress: {successfully_processed_repos} repos already processed")
        logger.info(f"Still need to process: {target_repos - successfully_processed_repos} repos")
        
        if successfully_processed_repos >= target_repos:
            logger.info(f"Target already reached! {successfully_processed_repos}/{target_repos} repos processed.")
            return
        
        current_start_index = 0
        batch_size = 50
        
        while successfully_processed_repos < target_repos:
            repos_batch = self.load_repos_from_file(
                language, 
                start_index=current_start_index, 
                batch_size=batch_size
            )
            
            if not repos_batch:
                logger.warning(f"No more repositories available. Processed {successfully_processed_repos}/{target_repos} repos.")
                break
            
            logger.info(f"Loaded batch of {len(repos_batch)} repos")
            logger.info(f"Progress: {successfully_processed_repos}/{target_repos} repos completed")
            
            for repo_idx, repo in enumerate(repos_batch):
                if successfully_processed_repos >= target_repos:
                    break
                
                try:
                    repo_full_name = f"{repo['owner']['login']}/{repo['name']}"
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
                    if repo_full_name in processed_repos_set:
                        logger.info(f"Skipping already processed repo: {repo_full_name}")
                        self.stats['repos_skipped_already_processed'] += 1
                        continue
                    
                    self.stats['total_repos_attempted'] += 1
                    logger.info(f"Processing repo {repo_idx + 1}/{len(repos_batch)}: {repo_full_name}")
                    
                    # è·å–PRåˆ—è¡¨
                    prs = self.get_pull_requests(
                        repo['owner']['login'], 
                        repo['name'], 
                        max_prs=max_prs_per_repo,
                        skip_if_too_many=200
                    )
                    
                    if not prs:
                        logger.warning(f"No PRs found for {repo_full_name}")
                        self.stats['repos_skipped_no_prs'] += 1
                        continue
                    
                    logger.info(f"Processing {len(prs)} PRs for {repo_full_name}")
                    
                    # å¤„ç†æ‰€æœ‰PR
                    repo_pr_count = 0
                    for pr_idx, pr in enumerate(prs):
                        logger.info(f"Processing PR {pr_idx + 1}/{len(prs)} (#{pr['number']})")
                        
                        if self.process_pr_data(repo, pr, language):
                            repo_pr_count += 1
                            self.stats['total_prs_processed'] += 1
                        
                        time.sleep(1)
                    
                    # åªè¦æˆåŠŸå¤„ç†äº†PRï¼Œå°±è®¡å…¥æˆåŠŸæ•°é‡
                    if repo_pr_count > 0:
                        successfully_processed_repos += 1
                        processed_repos_set.add(repo_full_name)
                        self.stats['repos_successfully_processed'] += 1
                        logger.info(f"âœ… Completed repo {repo_full_name}: processed {repo_pr_count} PRs")
                        logger.info(f"ğŸ¯ Progress: {successfully_processed_repos}/{target_repos} repos completed")
                    
                except Exception as e:
                    logger.error(f"Error processing repo {repo['name']}: {e}")
                    self.stats['repos_failed'] += 1
                    continue
            
            current_start_index += batch_size
            
            if successfully_processed_repos >= target_repos:
                break
            
            if len(repos_batch) < batch_size:
                break
        
        logger.info(f"Completed crawling {language}. Successfully processed {successfully_processed_repos}/{target_repos} repositories")
    
    def run(self, languages: List[str], target_repos_per_language: int = 200, max_prs_per_repo: int = None):
        """è¿è¡Œçˆ¬è™«"""
        logger.info(f"Starting multi-language GitHub PR crawler for languages: {languages}")
        
        for language in languages:
            try:
                self.crawl_language(language, target_repos_per_language, max_prs_per_repo)
                logger.info(f"Completed language: {language}")
                time.sleep(5)
            except Exception as e:
                logger.error(f"Failed to crawl language {language}: {e}")
                continue
        
        logger.info(f"Crawling completed. Total API calls: {self.api_calls}")
        self.print_statistics()
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("=== Multi-Language Crawling Statistics ===")
        logger.info(f"Total API calls: {self.api_calls}")
        logger.info(f"Remaining rate limit: {self.rate_limit_remaining}")
        logger.info(f"Total repos attempted: {self.stats['total_repos_attempted']}")
        logger.info(f"Repos successfully processed: {self.stats['repos_successfully_processed']}")
        logger.info(f"Repos skipped (already processed): {self.stats['repos_skipped_already_processed']}")
        logger.info(f"Repos skipped (no PRs): {self.stats['repos_skipped_no_prs']}")
        logger.info(f"Repos skipped (too many PRs): {self.stats['repos_skipped_too_many_prs']}")
        logger.info(f"Repos failed: {self.stats['repos_failed']}")
        logger.info(f"Total PRs processed: {self.stats['total_prs_processed']}")
        logger.info(f"Functions detected: {self.stats['functions_detected']}")
        logger.info(f"Classes detected: {self.stats['classes_detected']}")
        logger.info(f"Imports extracted: {self.stats['imports_extracted']}")
        logger.info(f"Language distribution: {self.stats['language_stats']}")

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    env_file = Path('.env')
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value
    
    github_token = os.getenv('GITHUB_TOKEN')
    if not github_token:
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GITHUB_TOKEN")
        return
    
    # è¦çˆ¬å–çš„ç¼–ç¨‹è¯­è¨€ï¼ˆæ”¯æŒå¤šç§è¯­è¨€ï¼‰
    languages = [
       'python',
       # 'javascript',
       # 'java',
       # 'typescript',
       # 'golang',
       # 'cpp'
    ]
    
    # åˆ›å»ºå¤šè¯­è¨€çˆ¬è™«å®ä¾‹
    crawler = MultiLanguageGitHubPRCrawler(
        token=github_token, 
        repos_dir="top_2000_star_repos_this_year",
        output_dir="github_pr_data"
    )
    
    # å¼€å§‹çˆ¬å–
    crawler.run(
        languages=languages,
        target_repos_per_language=100,
        max_prs_per_repo=None
    )

if __name__ == "__main__":
    main()